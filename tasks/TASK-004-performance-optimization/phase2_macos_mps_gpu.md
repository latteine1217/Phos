# Phase 2: macOS MPS GPU åŠ é€Ÿå¯¦æ–½è¨ˆç•«

**å‰µå»ºæ™‚é–“**: 2025-12-22  
**ç›®æ¨™å¹³å°**: macOS (Apple Silicon M1/M2/M3)  
**GPU å¾Œç«¯**: PyTorch MPS (Metal Performance Shaders)  
**é æœŸåŠ é€Ÿ**: 3-5x (M3 æ™¶ç‰‡)

---

## ğŸ“‹ Executive Summary

### å¹³å°é™åˆ¶èˆ‡æ–¹æ¡ˆèª¿æ•´

åŸè¨ˆç•«æ¡ç”¨ **CuPy**ï¼ˆNVIDIA CUDAï¼‰ï¼Œä½†åœ¨ macOS ä¸Šä¸å¯è¡Œï¼š
- âŒ macOS ä¸æ”¯æ´ NVIDIA CUDAï¼ˆè‡ª 10.14 èµ·ï¼‰
- âŒ CuPy åƒ…æ”¯æ´ NVIDIA GPU
- âœ… **æ›¿ä»£æ–¹æ¡ˆ**ï¼šPyTorch + MPS (Metal Performance Shaders)

### PyTorch MPS å„ªå‹¢

**Apple Silicon åŸç”Ÿæ”¯æ´**:
- âœ… M1/M2/M3 æ™¶ç‰‡å…§å»º GPUï¼ˆ8-40 æ ¸å¿ƒï¼‰
- âœ… çµ±ä¸€è¨˜æ†¶é«”æ¶æ§‹ï¼ˆCPU èˆ‡ GPU å…±äº«è¨˜æ†¶é«”ï¼Œå‚³è¼¸é–‹éŠ·å°ï¼‰
- âœ… Metal æ¡†æ¶åŸç”Ÿå„ªåŒ–
- âœ… å®‰è£ç°¡å–®ï¼ˆ`pip install torch`ï¼‰

**é æœŸæ•ˆèƒ½**:
| æ“ä½œ | CPU (M3) | MPS GPU (M3) | åŠ é€Ÿæ¯” |
|------|---------|--------------|--------|
| å–®æ¬¡å·ç© (201Ã—201) | 250ms | 60-80ms | **3-4x** |
| Halation (3é€šé“) | 1.4s | 350-450ms | **3-4x** |
| **ç«¯åˆ°ç«¯è™•ç†** | **2.1s** | **0.6-0.8s** | **2.5-3.5x** |

**æ³¨æ„**: MPS åŠ é€Ÿæ¯”ä½æ–¼ NVIDIA GPU (8-10x)ï¼Œä½†å° M3 æ™¶ç‰‡ç”¨æˆ¶ä»é¡¯è‘—æå‡ã€‚

---

## ğŸ¯ å¯¦æ–½è¨ˆç•«

### Phase 2.1: PyTorch MPS åŸºç¤æ•´åˆï¼ˆä¼°è¨ˆ 4-6 å°æ™‚ï¼‰

#### ç›®æ¨™
- å¯¦ä½œ `phos_gpu.py` æ¨¡çµ„ï¼ˆPyTorch MPS å¾Œç«¯ï¼‰
- è‡ªå‹•æª¢æ¸¬ MPS å¯ç”¨æ€§
- æ•´åˆåˆ° `convolve_adaptive()`
- æ¸¬è©¦ç²¾åº¦èˆ‡æ•ˆèƒ½

#### ä»»å‹™æ¸…å–®

**Task 2.1.1: å®‰è£èˆ‡é©—è­‰ PyTorch**
```bash
# å®‰è£ PyTorchï¼ˆmacOS ç‰ˆæœ¬ï¼ŒåŒ…å« MPS æ”¯æ´ï¼‰
pip install torch torchvision

# é©—è­‰ MPS å¯ç”¨æ€§
python3 -c "import torch; print(f'MPS available: {torch.backends.mps.is_available()}')"
python3 -c "import torch; print(f'MPS built: {torch.backends.mps.is_built()}')"
```

**é æœŸè¼¸å‡º**:
```
MPS available: True
MPS built: True
```

**Task 2.1.2: å‰µå»º `phos_gpu.py` æ¨¡çµ„**

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è‡ªå‹•æª¢æ¸¬ MPS/CUDA/CPU
2. å¯¦ä½œ `convolve_gpu()` (PyTorch å¾Œç«¯)
3. è™•ç† reflect paddingï¼ˆæ‰‹å‹• padï¼‰
4. å¯¦ä½œè¨˜æ†¶é«”ç®¡ç†èˆ‡éŒ¯èª¤è™•ç†

**æª”æ¡ˆçµæ§‹**:
```
phos_gpu.py
â”œâ”€â”€ GPU_BACKEND: 'mps' | 'cuda' | 'cpu'
â”œâ”€â”€ convolve_gpu(image, kernel) â†’ np.ndarray
â”œâ”€â”€ convolve_batch_gpu(images, kernel) â†’ list[np.ndarray]
â”œâ”€â”€ get_gpu_info() â†’ dict
â””â”€â”€ benchmark_gpu() â†’ dict
```

**Task 2.1.3: æ•´åˆåˆ°ä¸»ç¨‹å¼**

ä¿®æ”¹ `Phos_0.3.0.py`:
1. å°å…¥ `phos_gpu` æ¨¡çµ„ï¼ˆæ¢ä»¶å¼ï¼‰
2. ä¿®æ”¹ `convolve_adaptive()` æ·»åŠ  `use_gpu` åƒæ•¸
3. ä¿®æ”¹ `apply_halation()` æ”¯æ´ GPU
4. ä¿®æ”¹ `apply_bloom_mie_corrected()` æ”¯æ´ GPU

**Task 2.1.4: Streamlit UI æ•´åˆ**

æ·»åŠ  GPU æ§åˆ¶é¢æ¿ï¼š
- é¡¯ç¤º GPU è³‡è¨Šï¼ˆå‹è™Ÿã€å¯ç”¨æ€§ï¼‰
- ã€Œä½¿ç”¨ GPU åŠ é€Ÿã€æ ¸å–æ–¹å¡Š
- æ•ˆèƒ½ç›£æ§ï¼ˆGPU è¨˜æ†¶é«”ä½¿ç”¨ï¼‰

---

### Phase 2.2: æ•ˆèƒ½å„ªåŒ–èˆ‡æ¸¬è©¦ï¼ˆä¼°è¨ˆ 2-3 å°æ™‚ï¼‰

#### ä»»å‹™æ¸…å–®

**Task 2.2.1: æ•ˆèƒ½åŸºæº–æ¸¬è©¦**

å‰µå»º `scripts/benchmark_mps_gpu.py`:
- æ¸¬è©¦ä¸åŒå½±åƒå°ºå¯¸ï¼ˆ1000Ã—1500, 2000Ã—3000ï¼‰
- æ¸¬è©¦ä¸åŒæ ¸å¤§å°ï¼ˆ61Ã—61, 121Ã—121, 151Ã—151ï¼‰
- å°æ¯” CPU vs MPS
- æ¸¬è©¦æ‰¹æ¬¡è™•ç†ï¼ˆ1, 3, 10 å¼µï¼‰

**é©—æ”¶æ¨™æº–**:
- å–®å¼µå½±åƒåŠ é€Ÿ >3x
- æ‰¹æ¬¡è™•ç†åŠ é€Ÿ >4x
- ç¸½è™•ç†æ™‚é–“ <0.8s (2000Ã—3000)

**Task 2.2.2: ç²¾åº¦é©—è­‰**

æ¸¬è©¦ GPU vs CPU çµæœå·®ç•°ï¼š
- PSNR >40dBï¼ˆå¹¾ä¹ç„¡å·®ç•°ï¼‰
- æœ€å¤§åƒç´ èª¤å·® <0.01
- èƒ½é‡å®ˆæ†èª¤å·® <0.1%

**Task 2.2.3: éŒ¯èª¤è™•ç†æ¸¬è©¦**

æ¨¡æ“¬éŒ¯èª¤æƒ…å¢ƒï¼š
- MPS OOMï¼ˆè¨˜æ†¶é«”ä¸è¶³ï¼‰ â†’ è‡ªå‹• fallback CPU
- MPS ä¸å¯ç”¨ â†’ è‡ªå‹•ä½¿ç”¨ CPU
- å½±åƒå°ºå¯¸éå¤§ â†’ è­¦å‘Šä¸¦ä½¿ç”¨ CPU

---

### Phase 2.3: æ–‡æª”èˆ‡ç”¨æˆ¶æŒ‡å¼•ï¼ˆä¼°è¨ˆ 1-2 å°æ™‚ï¼‰

#### ä»»å‹™æ¸…å–®

**Task 2.3.1: æ›´æ–° README.md**

æ·»åŠ ç« ç¯€ï¼š
- GPU åŠ é€Ÿéœ€æ±‚ï¼ˆmacOS: M1/M2/M3ï¼‰
- å®‰è£ PyTorch æŒ‡å¼•
- æ•ˆèƒ½é æœŸï¼ˆmacOS vs Linux/Windowsï¼‰
- å¸¸è¦‹å•é¡Œï¼ˆFAQï¼‰

**Task 2.3.2: å‰µå»º GPU å„ªåŒ–æ–‡æª”**

`docs/GPU_ACCELERATION_GUIDE.md`:
- æ”¯æ´çš„å¹³å°èˆ‡ GPU
- å®‰è£æŒ‡å—ï¼ˆmacOS/Linux/Windowsï¼‰
- æ•ˆèƒ½å°æ¯”è¡¨
- å•é¡Œæ’æŸ¥

---

## ğŸ’» ç¨‹å¼ç¢¼å¯¦ä½œ

### 1. `phos_gpu.py` å®Œæ•´å¯¦ä½œ

```python
"""
Phos GPU åŠ é€Ÿæ¨¡çµ„
æ”¯æ´: PyTorch (MPS/CUDA), CuPy (CUDA)
"""

import numpy as np
import warnings
from typing import Optional, Union, List, Tuple

# ===== GPU å¾Œç«¯åµæ¸¬ =====

# å˜—è©¦å°å…¥ PyTorch
try:
    import torch
    import torch.nn.functional as F
    
    # åµæ¸¬å¯ç”¨è£ç½®
    if torch.backends.mps.is_available() and torch.backends.mps.is_built():
        GPU_BACKEND = 'mps'
        GPU_DEVICE = torch.device('mps')
    elif torch.cuda.is_available():
        GPU_BACKEND = 'cuda'
        GPU_DEVICE = torch.device('cuda')
    else:
        GPU_BACKEND = 'cpu'
        GPU_DEVICE = torch.device('cpu')
    
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    GPU_BACKEND = 'cpu'
    GPU_DEVICE = None

# å˜—è©¦å°å…¥ CuPyï¼ˆåƒ… NVIDIA GPUï¼‰
try:
    import cupy as cp
    import cupyx.scipy.ndimage as cpx_ndimage
    CUPY_AVAILABLE = True
except ImportError:
    CUPY_AVAILABLE = False
    cp = None


# ===== GPU è³‡è¨ŠæŸ¥è©¢ =====

def get_gpu_info() -> dict:
    """
    ç²å– GPU è³‡è¨Š
    
    Returns:
        dict: {
            'available': bool,
            'backend': 'mps' | 'cuda' | 'cpu',
            'device_name': str,
            'memory_total_gb': float,  # CUDA only
            'memory_free_gb': float    # CUDA only
        }
    """
    if not PYTORCH_AVAILABLE:
        return {
            'available': False,
            'backend': 'cpu',
            'reason': 'PyTorch not installed'
        }
    
    if GPU_BACKEND == 'mps':
        # MPS ä¸æä¾›è¨˜æ†¶é«”æŸ¥è©¢ API
        return {
            'available': True,
            'backend': 'mps',
            'device_name': 'Apple Metal GPU',
            'memory_total_gb': None,
            'memory_free_gb': None
        }
    elif GPU_BACKEND == 'cuda':
        try:
            device_id = torch.cuda.current_device()
            device_name = torch.cuda.get_device_name(device_id)
            memory_total = torch.cuda.get_device_properties(device_id).total_memory / 1e9
            memory_free = (torch.cuda.get_device_properties(device_id).total_memory - 
                          torch.cuda.memory_allocated(device_id)) / 1e9
            
            return {
                'available': True,
                'backend': 'cuda',
                'device_name': device_name,
                'memory_total_gb': memory_total,
                'memory_free_gb': memory_free
            }
        except Exception as e:
            return {
                'available': False,
                'backend': 'cuda',
                'reason': str(e)
            }
    else:
        return {
            'available': False,
            'backend': 'cpu',
            'reason': 'No GPU available'
        }


# ===== PyTorch GPU å·ç© =====

def convolve_pytorch(
    image_np: np.ndarray, 
    kernel_np: np.ndarray,
    device: Optional[torch.device] = None
) -> np.ndarray:
    """
    PyTorch GPU åŠ é€Ÿå·ç©ï¼ˆæ”¯æ´ MPS/CUDAï¼‰
    
    ä½¿ç”¨ reflect padding ä¿æŒç‰©ç†ä¸€è‡´æ€§
    
    Args:
        image_np: NumPy é™£åˆ— (H, W), float32
        kernel_np: å·ç©æ ¸ (kH, kW), float32
        device: torch.device ('mps'/'cuda'/'cpu')ï¼ŒNone = è‡ªå‹•é¸æ“‡
        
    Returns:
        result_np: å·ç©çµæœ (H, W), float32
    """
    if device is None:
        device = GPU_DEVICE
    
    try:
        # 1. NumPy â†’ Torch (CPU)
        # (H, W) â†’ (1, 1, H, W)
        image_t = torch.from_numpy(image_np).unsqueeze(0).unsqueeze(0)
        kernel_t = torch.from_numpy(kernel_np).unsqueeze(0).unsqueeze(0)
        
        # 2. å‚³è¼¸åˆ° GPU
        image_t = image_t.to(device)
        kernel_t = kernel_t.to(device)
        
        # 3. Reflect paddingï¼ˆæ‰‹å‹•å¯¦ä½œï¼Œä¿æŒèˆ‡ OpenCV ä¸€è‡´ï¼‰
        kh, kw = kernel_np.shape
        pad_h, pad_w = kh // 2, kw // 2
        
        image_padded = F.pad(
            image_t,
            (pad_w, pad_w, pad_h, pad_h),  # (left, right, top, bottom)
            mode='reflect'
        )
        
        # 4. å·ç©ï¼ˆé—œé–‰è‡ªå‹•å¾®åˆ†ä»¥æå‡æ•ˆèƒ½ï¼‰
        with torch.no_grad():
            result_t = F.conv2d(image_padded, kernel_t, padding=0)
        
        # 5. GPU â†’ CPU â†’ NumPy
        result_np = result_t.squeeze().cpu().numpy()
        
        return result_np
    
    except RuntimeError as e:
        # MPS OOM æˆ–å…¶ä»–éŒ¯èª¤ â†’ fallback CPU
        warnings.warn(f"GPU å·ç©å¤±æ•—ï¼Œfallback CPU: {e}")
        import cv2
        return cv2.filter2D(image_np, -1, kernel_np, borderType=cv2.BORDER_REFLECT)


def convolve_batch_pytorch(
    images_np: List[np.ndarray],
    kernel_np: np.ndarray,
    device: Optional[torch.device] = None
) -> List[np.ndarray]:
    """
    æ‰¹æ¬¡ GPU å·ç©ï¼ˆæœ€å¤§åŒ– GPU åˆ©ç”¨ç‡ï¼‰
    
    Args:
        images_np: åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ ç‚º (H, W) NumPy é™£åˆ—
        kernel_np: å…±ç”¨å·ç©æ ¸ (kH, kW)
        device: torch.device
        
    Returns:
        results_np: åˆ—è¡¨ï¼Œå·ç©çµæœ
    """
    if device is None:
        device = GPU_DEVICE
    
    try:
        # 1. æ‰¹æ¬¡è½‰æ› (N, 1, H, W)
        images_t = torch.stack([
            torch.from_numpy(img).unsqueeze(0) for img in images_np
        ]).to(device)
        
        # 2. æ ¸è½‰æ› (1, 1, kH, kW)
        kernel_t = torch.from_numpy(kernel_np).unsqueeze(0).unsqueeze(0).to(device)
        
        # 3. Reflect padding
        kh, kw = kernel_np.shape
        pad_h, pad_w = kh // 2, kw // 2
        images_padded = F.pad(images_t, (pad_w, pad_w, pad_h, pad_h), mode='reflect')
        
        # 4. æ‰¹æ¬¡å·ç©ï¼ˆä¸€æ¬¡ GPU èª¿ç”¨è™•ç†æ‰€æœ‰å½±åƒï¼‰
        with torch.no_grad():
            results_t = F.conv2d(images_padded, kernel_t, padding=0)
        
        # 5. æ‰¹æ¬¡è¿”å›
        results_np = [r.squeeze().cpu().numpy() for r in results_t]
        
        return results_np
    
    except RuntimeError as e:
        warnings.warn(f"æ‰¹æ¬¡ GPU å·ç©å¤±æ•—ï¼Œfallback é€å¼µè™•ç†: {e}")
        return [convolve_pytorch(img, kernel_np, device) for img in images_np]


# ===== CuPy GPU å·ç©ï¼ˆNVIDIA CUDA Onlyï¼‰=====

def convolve_cupy(image_np: np.ndarray, kernel_np: np.ndarray) -> np.ndarray:
    """
    CuPy GPU åŠ é€Ÿå·ç©ï¼ˆåƒ… NVIDIA GPUï¼‰
    
    Args:
        image_np: NumPy é™£åˆ— (H, W), float32
        kernel_np: å·ç©æ ¸ (kH, kW), float32
        
    Returns:
        result_np: å·ç©çµæœ (H, W), float32
    """
    if not CUPY_AVAILABLE:
        raise RuntimeError("CuPy not available (requires NVIDIA GPU + CUDA)")
    
    try:
        # 1. å‚³è¼¸åˆ° GPU
        image_gpu = cp.asarray(image_np)
        kernel_gpu = cp.asarray(kernel_np)
        
        # 2. GPU å·ç©ï¼ˆmode='reflect'ï¼‰
        result_gpu = cpx_ndimage.convolve(image_gpu, kernel_gpu, mode='reflect')
        
        # 3. å‚³å› CPU
        result_np = cp.asnumpy(result_gpu)
        
        return result_np
    
    except Exception as e:
        warnings.warn(f"CuPy å·ç©å¤±æ•—ï¼Œfallback CPU: {e}")
        import cv2
        return cv2.filter2D(image_np, -1, kernel_np, borderType=cv2.BORDER_REFLECT)


# ===== çµ±ä¸€ä»‹é¢ =====

def convolve_gpu(
    image: np.ndarray,
    kernel: np.ndarray,
    backend: str = 'auto'
) -> np.ndarray:
    """
    GPU åŠ é€Ÿå·ç©ï¼ˆçµ±ä¸€ä»‹é¢ï¼‰
    
    è‡ªå‹•é¸æ“‡æœ€ä½³ GPU å¾Œç«¯ï¼š
    - macOS M1/M2/M3: PyTorch MPS
    - NVIDIA GPU: CuPy (å„ªå…ˆ) æˆ– PyTorch CUDA
    - å…¶ä»–: CPU fallback
    
    Args:
        image: NumPy é™£åˆ— (H, W), float32
        kernel: å·ç©æ ¸ (kH, kW), float32
        backend: 'auto' | 'pytorch' | 'cupy' | 'cpu'
        
    Returns:
        result: å·ç©çµæœ (H, W), float32
    """
    if backend == 'auto':
        if GPU_BACKEND == 'mps' and PYTORCH_AVAILABLE:
            return convolve_pytorch(image, kernel)
        elif GPU_BACKEND == 'cuda' and CUPY_AVAILABLE:
            return convolve_cupy(image, kernel)
        elif GPU_BACKEND == 'cuda' and PYTORCH_AVAILABLE:
            return convolve_pytorch(image, kernel)
        else:
            # Fallback CPU
            import cv2
            return cv2.filter2D(image, -1, kernel, borderType=cv2.BORDER_REFLECT)
    
    elif backend == 'pytorch':
        return convolve_pytorch(image, kernel)
    
    elif backend == 'cupy':
        return convolve_cupy(image, kernel)
    
    elif backend == 'cpu':
        import cv2
        return cv2.filter2D(image, -1, kernel, borderType=cv2.BORDER_REFLECT)
    
    else:
        raise ValueError(f"Unknown backend: {backend}")


# ===== æ•ˆèƒ½åŸºæº–æ¸¬è©¦ =====

def benchmark_gpu(image_size: Tuple[int, int] = (2000, 3000), 
                  kernel_size: int = 201) -> dict:
    """
    GPU vs CPU æ•ˆèƒ½åŸºæº–æ¸¬è©¦
    
    Args:
        image_size: å½±åƒå°ºå¯¸ (H, W)
        kernel_size: æ ¸å¤§å°
        
    Returns:
        dict: {
            'cpu_time_ms': float,
            'gpu_time_ms': float,
            'speedup': float,
            'gpu_backend': str
        }
    """
    import time
    import cv2
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    image = np.random.rand(*image_size).astype(np.float32)
    kernel = cv2.getGaussianKernel(kernel_size, kernel_size / 6.0)
    kernel = (kernel @ kernel.T).astype(np.float32)
    
    # Warmup
    if PYTORCH_AVAILABLE:
        _ = convolve_pytorch(image, kernel)
    
    # CPU æ¸¬è©¦
    t0 = time.perf_counter()
    result_cpu = cv2.filter2D(image, -1, kernel, borderType=cv2.BORDER_REFLECT)
    cpu_time = (time.perf_counter() - t0) * 1000
    
    # GPU æ¸¬è©¦
    if GPU_BACKEND != 'cpu':
        t0 = time.perf_counter()
        result_gpu = convolve_gpu(image, kernel)
        gpu_time = (time.perf_counter() - t0) * 1000
        
        # ç²¾åº¦é©—è­‰
        mse = np.mean((result_cpu - result_gpu) ** 2)
        psnr = 10 * np.log10(1.0 / mse) if mse > 0 else float('inf')
        
        return {
            'cpu_time_ms': cpu_time,
            'gpu_time_ms': gpu_time,
            'speedup': cpu_time / gpu_time,
            'gpu_backend': GPU_BACKEND,
            'psnr_db': psnr
        }
    else:
        return {
            'cpu_time_ms': cpu_time,
            'gpu_time_ms': None,
            'speedup': 1.0,
            'gpu_backend': 'cpu',
            'psnr_db': None
        }


# ===== æ¨¡çµ„åˆå§‹åŒ–è³‡è¨Š =====

if __name__ == "__main__":
    print("=" * 60)
    print("Phos GPU åŠ é€Ÿæ¨¡çµ„")
    print("=" * 60)
    
    gpu_info = get_gpu_info()
    print(f"\nGPU ç‹€æ…‹:")
    print(f"  å¯ç”¨: {gpu_info['available']}")
    print(f"  å¾Œç«¯: {gpu_info.get('backend', 'N/A')}")
    print(f"  è£ç½®: {gpu_info.get('device_name', 'N/A')}")
    
    if gpu_info['available']:
        print(f"\nåŸ·è¡ŒåŸºæº–æ¸¬è©¦ (2000Ã—3000, 201Ã—201 æ ¸)...")
        results = benchmark_gpu()
        print(f"\nçµæœ:")
        print(f"  CPU æ™‚é–“: {results['cpu_time_ms']:.1f} ms")
        if results['gpu_time_ms']:
            print(f"  GPU æ™‚é–“: {results['gpu_time_ms']:.1f} ms")
            print(f"  åŠ é€Ÿæ¯”: {results['speedup']:.2f}x")
            print(f"  ç²¾åº¦ (PSNR): {results['psnr_db']:.1f} dB")
    
    print("=" * 60)
```

---

### 2. æ•´åˆåˆ° `Phos_0.3.0.py`

**ä¿®æ”¹é» 1: é ‚éƒ¨å°å…¥**

```python
# Phos_0.3.0.py (Line ~30)

# GPU åŠ é€Ÿæ¨¡çµ„ï¼ˆå¯é¸ï¼‰
try:
    from phos_gpu import (
        GPU_BACKEND, 
        convolve_gpu, 
        get_gpu_info,
        PYTORCH_AVAILABLE
    )
    GPU_AVAILABLE = (GPU_BACKEND in ['mps', 'cuda'])
except ImportError:
    GPU_AVAILABLE = False
    GPU_BACKEND = 'cpu'
```

**ä¿®æ”¹é» 2: ä¿®æ”¹ `convolve_adaptive()`**

```python
# Phos_0.3.0.py (Line ~1330)

def convolve_adaptive(
    image: np.ndarray, 
    kernel: np.ndarray, 
    method: str = 'auto',
    use_gpu: bool = False  # æ–°å¢åƒæ•¸
) -> np.ndarray:
    """
    è‡ªé©æ‡‰é¸æ“‡å·ç©æ–¹æ³•ï¼ˆæ”¯æ´ GPU åŠ é€Ÿï¼‰
    
    Args:
        image: è¼¸å…¥å½±åƒ (H, W)
        kernel: å·ç©æ ¸ (kH, kW)
        method: 'auto' | 'spatial' | 'fft' | 'gpu'
        use_gpu: æ˜¯å¦å˜—è©¦ä½¿ç”¨ GPUï¼ˆéœ€ç¡¬é«”æ”¯æ´ï¼‰
        
    Returns:
        å·ç©çµæœ (H, W)
    """
    # GPU è·¯å¾‘ï¼ˆå„ªå…ˆï¼‰
    if use_gpu and GPU_AVAILABLE:
        return convolve_gpu(image, kernel)
    
    # CPU è·¯å¾‘ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
    if method == 'auto':
        ksize = kernel.shape[0]
        if ksize > 150:
            return convolve_fft(image, kernel)
        else:
            return cv2.filter2D(image, -1, kernel, borderType=cv2.BORDER_REFLECT)
    elif method == 'spatial':
        return cv2.filter2D(image, -1, kernel, borderType=cv2.BORDER_REFLECT)
    elif method == 'fft':
        return convolve_fft(image, kernel)
    elif method == 'gpu':
        if not GPU_AVAILABLE:
            raise RuntimeError("GPU not available, use method='auto' for fallback")
        return convolve_gpu(image, kernel)
    else:
        raise ValueError(f"Unknown method: {method}")
```

**ä¿®æ”¹é» 3: ä¿®æ”¹ `apply_halation()` æ”¯æ´ GPU**

```python
# Phos_0.3.0.py (Line ~1545)

def apply_halation(
    lux: np.ndarray, 
    halation_params, 
    wavelength: float = 550.0,
    use_gpu: bool = False  # æ–°å¢åƒæ•¸
) -> np.ndarray:
    """
    æ‡‰ç”¨ Halationï¼ˆèƒŒå±¤åå°„ï¼‰æ•ˆæœ
    
    Args:
        lux: å…‰åº¦é€šé“æ•¸æ“š (0-1 ç¯„åœ)
        halation_params: HalationParams å°è±¡
        wavelength: ç•¶å‰é€šé“çš„æ³¢é•·ï¼ˆnmï¼‰
        use_gpu: æ˜¯å¦ä½¿ç”¨ GPU åŠ é€Ÿ
        
    Returns:
        æ‡‰ç”¨ Halation å¾Œçš„å…‰åº¦æ•¸æ“š
    """
    # ... (å‰é¢é‚è¼¯ä¸è®Š)
    
    if halation_params.psf_type == "exponential":
        sigma_base = halation_params.psf_radius * 0.2
        sigma_small = sigma_base
        sigma_medium = sigma_base * 2.0
        sigma_large = sigma_base * 4.0
        
        ksize_small = 61
        ksize_medium = 121
        ksize_large = 151
        
        # ===== GPU åŠ é€Ÿè·¯å¾‘ =====
        if use_gpu and GPU_AVAILABLE:
            from phos_gpu import convolve_pytorch
            
            # ä½¿ç”¨ PyTorchï¼Œä¸€æ¬¡å‚³è¼¸ 3 å€‹æ ¸
            kernel_small = get_gaussian_kernel(sigma_small, ksize_small)
            kernel_medium = get_gaussian_kernel(sigma_medium, ksize_medium)
            kernel_large = get_gaussian_kernel(sigma_large, ksize_large)
            
            halation_layer = (
                convolve_pytorch(halation_energy, kernel_small) * 0.5 +
                convolve_pytorch(halation_energy, kernel_medium) * 0.3 +
                convolve_pytorch(halation_energy, kernel_large) * 0.2
            )
        else:
            # CPU è·¯å¾‘ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
            halation_layer = (
                cv2.GaussianBlur(halation_energy, (ksize_small, ksize_small), sigma_small) * 0.5 +
                cv2.GaussianBlur(halation_energy, (ksize_medium, ksize_medium), sigma_medium) * 0.3 +
                cv2.GaussianBlur(halation_energy, (ksize_large, ksize_large), sigma_large) * 0.2
            )
    
    # ... (å¾ŒçºŒé‚è¼¯ä¸è®Š)
```

---

### 3. Streamlit UI æ•´åˆ

**ä¿®æ”¹ `Phos_0.3.0.py` ä¸»ç¨‹å¼**

```python
# Phos_0.3.0.py (Line ~1800, Streamlit å´é‚Šæ¬„)

def main():
    # ... (å‰é¢é‚è¼¯)
    
    # ===== GPU åŠ é€Ÿæ§åˆ¶é¢æ¿ =====
    st.sidebar.markdown("---")
    st.sidebar.subheader("âš¡ GPU åŠ é€Ÿ")
    
    gpu_info = get_gpu_info()
    
    if gpu_info['available']:
        # é¡¯ç¤º GPU è³‡è¨Š
        backend_name = {
            'mps': 'Apple Metal (MPS)',
            'cuda': 'NVIDIA CUDA',
            'cpu': 'CPU Only'
        }.get(gpu_info['backend'], 'Unknown')
        
        st.sidebar.success(f"ğŸš€ GPU å¯ç”¨: {backend_name}")
        st.sidebar.caption(f"è£ç½®: {gpu_info['device_name']}")
        
        # GPU é–‹é—œ
        use_gpu = st.sidebar.checkbox(
            "å•Ÿç”¨ GPU åŠ é€Ÿ", 
            value=True,
            help="ä½¿ç”¨ GPU åŠ é€Ÿå·ç©é‹ç®—ï¼ˆHalation, Bloomï¼‰\né æœŸåŠ é€Ÿ: 3-5x (macOS M3)"
        )
        
        # é¡¯ç¤ºæ•ˆèƒ½æç¤º
        if use_gpu:
            st.sidebar.info("ğŸ’¡ GPU æ¨¡å¼ï¼šé æœŸè™•ç†æ™‚é–“ 0.6-0.8 ç§’ (2000Ã—3000)")
    else:
        st.sidebar.warning("ğŸ’» GPU ä¸å¯ç”¨")
        st.sidebar.caption(f"åŸå› : {gpu_info.get('reason', 'Unknown')}")
        
        if GPU_BACKEND == 'cpu' and not PYTORCH_AVAILABLE:
            st.sidebar.info("ğŸ“¦ å®‰è£ PyTorch ä»¥å•Ÿç”¨ GPU åŠ é€Ÿ:\n```\npip install torch torchvision\n```")
        
        use_gpu = False
    
    # ... (å¾ŒçºŒè™•ç†é‚è¼¯ä¸­å‚³é use_gpu åƒæ•¸)
    
    # ç¯„ä¾‹ï¼šè™•ç†å½±åƒæ™‚å‚³é use_gpu
    if st.button("è™•ç†å½±åƒ"):
        with st.spinner("è™•ç†ä¸­..."):
            # ... (å…¶ä»–è™•ç†é‚è¼¯)
            
            # æ‡‰ç”¨ Halationï¼ˆä½¿ç”¨ GPUï¼‰
            if film.halation_params.enabled:
                lux_r = apply_halation(lux_r, film.halation_params, wavelength=650.0, use_gpu=use_gpu)
                lux_g = apply_halation(lux_g, film.halation_params, wavelength=550.0, use_gpu=use_gpu)
                lux_b = apply_halation(lux_b, film.halation_params, wavelength=450.0, use_gpu=use_gpu)
```

---

## ğŸ“Š é©—æ”¶æ¨™æº–

### åŠŸèƒ½é©—æ”¶

- [ ] MPS GPU è‡ªå‹•æª¢æ¸¬æˆåŠŸ
- [ ] `convolve_gpu()` æ­£å¸¸é‹ä½œ
- [ ] Streamlit UI é¡¯ç¤º GPU ç‹€æ…‹
- [ ] GPU é–‹é—œå¯æ­£å¸¸åˆ‡æ›
- [ ] GPU å¤±æ•—æ™‚è‡ªå‹• fallback CPU

### æ•ˆèƒ½é©—æ”¶ (M3 æ™¶ç‰‡)

- [ ] å–®æ¬¡å·ç© (201Ã—201, 2000Ã—3000): <80ms (vs CPU 250ms, >3x)
- [ ] Halation (3é€šé“): <450ms (vs CPU 1.4s, >3x)
- [ ] ç«¯åˆ°ç«¯è™•ç†: <0.8s (vs CPU 2.1s, >2.5x)

### ç²¾åº¦é©—æ”¶

- [ ] GPU vs CPU PSNR >40dB
- [ ] æœ€å¤§åƒç´ èª¤å·® <0.01
- [ ] èƒ½é‡å®ˆæ†èª¤å·® <0.1%

### ç©©å®šæ€§é©—æ”¶

- [ ] é€£çºŒè™•ç† 10 å¼µå½±åƒç„¡éŒ¯èª¤
- [ ] è¨˜æ†¶é«”ç„¡æ´©æ¼ï¼ˆé•·æ™‚é–“é‹è¡Œï¼‰
- [ ] æ‰¹æ¬¡è™•ç†èˆ‡å–®å¼µè™•ç†çµæœä¸€è‡´

---

## âš ï¸ å·²çŸ¥é™åˆ¶èˆ‡é¢¨éšª

### é™åˆ¶

1. **MPS åŠ é€Ÿæ¯”ä½æ–¼ NVIDIA CUDA**
   - M3: 3-4x
   - NVIDIA RTX 3090: 8-10x
   - åŸå› ï¼šMPS å„ªåŒ–è¼ƒå°‘ï¼ŒMetal API é™åˆ¶

2. **é¦–æ¬¡èª¿ç”¨é–‹éŠ·**
   - PyTorch éœ€åˆå§‹åŒ– GPUï¼ˆ~100-200msï¼‰
   - å¾ŒçºŒèª¿ç”¨ç„¡æ­¤é–‹éŠ·

3. **è¨˜æ†¶é«”é™åˆ¶**
   - çµ±ä¸€è¨˜æ†¶é«”æ¶æ§‹ï¼šèˆ‡ç³»çµ±å…±äº« RAM
   - M3 (8GB å‹è™Ÿ): å¯èƒ½ OOMï¼ˆå¤§æ‰¹æ¬¡ï¼‰

### é¢¨éšª

1. **PyTorch ç‰ˆæœ¬ç›¸å®¹æ€§**
   - MPS æ”¯æ´åœ¨ PyTorch 1.12+ æ‰ç©©å®š
   - éœ€è¦ macOS 12.3+

2. **Metal API é™åˆ¶**
   - æŸäº›æ“ä½œå¯èƒ½ç„¡ MPS å¯¦ä½œ
   - Fallback CPU æ™‚æ•ˆèƒ½é€€åŒ–

---

## ğŸ“‹ æ™‚ç¨‹è¦åŠƒ

### Week 1 (Day 1-2)
- [x] åˆ¶å®šå¯¦æ–½è¨ˆç•«ï¼ˆæœ¬æ–‡ä»¶ï¼‰
- [ ] å®‰è£ PyTorch ä¸¦é©—è­‰ MPS
- [ ] å¯¦ä½œ `phos_gpu.py` åŸºç¤ç‰ˆæœ¬
- [ ] åŸºæº–æ¸¬è©¦ï¼ˆå–®æ¬¡å·ç©ï¼‰

### Week 1 (Day 3-4)
- [ ] æ•´åˆåˆ° `Phos_0.3.0.py`
- [ ] Streamlit UI æ•´åˆ
- [ ] ç«¯åˆ°ç«¯æ¸¬è©¦
- [ ] ç²¾åº¦é©—è­‰

### Week 1 (Day 5)
- [ ] æ•ˆèƒ½å„ªåŒ–ï¼ˆæ‰¹æ¬¡è™•ç†ï¼‰
- [ ] éŒ¯èª¤è™•ç†å®Œå–„
- [ ] æ–‡æª”æ›´æ–°

### Week 2 (Day 6-7)
- [ ] ç”¨æˆ¶æ¸¬è©¦ï¼ˆå¯¦éš›å½±åƒï¼‰
- [ ] Bug ä¿®å¾©
- [ ] æœ€çµ‚æ–‡æª”æ•´ç†

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡Œå‹•

### ç«‹å³åŸ·è¡Œ (ä»Šå¤©)

1. **å®‰è£ PyTorch**
```bash
pip install torch torchvision
python3 -c "import torch; print(torch.backends.mps.is_available())"
```

2. **å‰µå»º `phos_gpu.py`**
   - è¤‡è£½ä¸Šæ–¹å®Œæ•´ç¨‹å¼ç¢¼
   - æ¸¬è©¦ MPS å¯ç”¨æ€§

3. **åŸºæº–æ¸¬è©¦**
```bash
python3 phos_gpu.py
```

### å¾ŒçºŒåŸ·è¡Œ (æœ¬é€±)

4. **æ•´åˆåˆ°ä¸»ç¨‹å¼**
   - ä¿®æ”¹ `convolve_adaptive()`
   - ä¿®æ”¹ `apply_halation()`

5. **UI æ•´åˆ**
   - Streamlit å´é‚Šæ¬„æ·»åŠ  GPU æ§åˆ¶

6. **ç«¯åˆ°ç«¯æ¸¬è©¦**
```bash
streamlit run Phos_0.3.0.py
```

---

**ç‹€æ…‹**: ğŸ“ è¨ˆç•«å®Œæˆï¼Œç­‰å¾…åŸ·è¡Œ  
**é æœŸå®Œæˆæ™‚é–“**: 1 é€±  
**è² è²¬äºº**: Main Agent  
**æ–‡æª”ç‰ˆæœ¬**: v1.0
