#!/usr/bin/env python3
"""
Mie æ•£å°„æŸ¥è¡¨å…¨é¢é©—è­‰è…³æœ¬ï¼ˆCritical Task #3ï¼‰

æ ¹æ“šå¯«ç¨‹å¼å“²å­¸ã€ŒData â‰  Truthã€åŸå‰‡ï¼Œé©—è­‰ï¼š
1. æ’å€¼èª¤å·®çš„çµ±è¨ˆåˆ†å¸ƒï¼ˆå¹³å‡/ä¸­ä½æ•¸/95%/æœ€å¤§å€¼ï¼‰
2. æ¥µç«¯æƒ…æ³èª¤å·®ï¼ˆISO 6400, Î»=700nm ç­‰é‚Šç•Œé»ï¼‰
3. èƒ½é‡å®ˆæ†é©—è­‰ï¼ˆÎ· + Ï â‰¤ 1ï¼‰
4. å–®èª¿æ€§é©—è­‰ï¼ˆISO å¢åŠ  â†’ Î· å¢åŠ ï¼‰

ä¾è³´:
    pip install miepython numpy scipy matplotlib

ä½¿ç”¨:
    python3 scripts/validate_mie_lookup_comprehensive.py

è¼¸å‡º:
    - çµ‚ç«¯è¼¸å‡ºçµ±è¨ˆå ±å‘Š
    - validation_report.txtï¼ˆè©³ç´°å ±å‘Šï¼‰
    - error_distribution.pngï¼ˆèª¤å·®åˆ†å¸ƒåœ–ï¼‰
"""

import numpy as np
import miepython
from scipy.optimize import minimize
from scipy.stats import lognorm
from pathlib import Path
import sys
import matplotlib.pyplot as plt

# ============================================================
# 1. è¼‰å…¥æŸ¥è¡¨æ•¸æ“š
# ============================================================

def load_mie_lookup():
    """è¼‰å…¥ Mie æ•£å°„æŸ¥è¡¨"""
    lookup_path = Path(__file__).parent.parent / "data" / "mie_lookup_table_v3.npz"
    
    if not lookup_path.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæŸ¥è¡¨æ–‡ä»¶ä¸å­˜åœ¨ {lookup_path}")
        sys.exit(1)
    
    data = np.load(lookup_path)
    
    return {
        'wavelengths': data['wavelengths'],  # (10,) 400-700nm
        'iso_values': data['iso_values'],     # (20,) 50-6400
        'sigma_core': data['sigma_core'],     # (10, 20)
        'kappa_tail': data['kappa_tail'],     # (10, 20)
        'core_ratio': data['core_ratio'],     # (10, 20)
        'eta_scatter': data['eta_scatter']    # (10, 20)
    }


# ============================================================
# 2. ç‰©ç†åƒæ•¸å®šç¾©ï¼ˆèˆ‡ generate_mie_lookup.py ä¿æŒä¸€è‡´ï¼‰
# ============================================================

N_GELATIN = 1.50

def n_AgBr_vacuum(wavelength_nm):
    """AgBr æŠ˜å°„ç‡ï¼ˆCauchy å…¬å¼ï¼‰"""
    Î»_um = wavelength_nm / 1000.0
    A = 2.0393
    B = 0.0629
    return A + B / (Î»_um ** 2)

def relative_refractive_index(wavelength_nm):
    """ç›¸å°æŠ˜å°„ç‡ m = n_AgBr / n_gelatin"""
    return n_AgBr_vacuum(wavelength_nm) / N_GELATIN

def get_particle_distribution(iso):
    """ç²’å¾‘åˆ†å¸ƒåƒæ•¸ï¼ˆç·šæ€§æ’å€¼ï¼‰"""
    iso_list = [50, 100, 125, 160, 200, 250, 320, 400, 500, 640, 
                800, 1000, 1250, 1600, 2000, 2500, 3200, 4000, 5000, 6400]
    mean_list = [0.7, 0.8, 0.85, 0.9, 1.0, 1.05, 1.1, 1.2, 1.3, 1.4,
                 1.5, 1.6, 1.7, 1.8, 1.95, 2.1, 2.2, 2.35, 2.45, 2.5]
    std_list = [0.28, 0.30, 0.32, 0.33, 0.35, 0.36, 0.38, 0.40, 0.42, 0.43,
                0.45, 0.46, 0.48, 0.50, 0.52, 0.53, 0.55, 0.57, 0.58, 0.60]
    
    mean = np.interp(iso, iso_list, mean_list)
    std = np.interp(iso, iso_list, std_list)
    
    return {'mean': mean, 'std': std}


def compute_mie_parameters_ground_truth(wavelength_nm, iso):
    """
    è¨ˆç®—çœŸå¯¦ Mie åƒæ•¸ï¼ˆGround Truthï¼‰
    
    ä½¿ç”¨èˆ‡ generate_mie_lookup.py ç›¸åŒçš„æ¼”ç®—æ³•ï¼Œç¢ºä¿ä¸€è‡´æ€§
    """
    m = relative_refractive_index(wavelength_nm)
    dist = get_particle_distribution(iso)
    
    # å°æ•¸å¸¸æ…‹åˆ†å¸ƒæ¡æ¨£ï¼ˆ100 é»è¶³å¤ ç²¾ç¢ºï¼‰
    radii = np.linspace(0.1, 5.0, 100)  # Î¼m
    mu = np.log(dist['mean']) - 0.5 * np.log(1 + (dist['std'] / dist['mean']) ** 2)
    sigma = np.sqrt(np.log(1 + (dist['std'] / dist['mean']) ** 2))
    weights = lognorm.pdf(radii, sigma, scale=np.exp(mu))
    weights /= np.sum(weights)
    
    # è¨ˆç®—å¹³å‡æ•£å°„åƒæ•¸
    eta_total = 0
    rho_total = 0
    
    for r, w in zip(radii, weights):
        x = 2 * np.pi * r / (wavelength_nm / 1000.0)
        qext, qsca, qback, g = miepython.mie(m, x)
        
        eta = qsca  # æ•£å°„æ•ˆç‡
        rho = qext - qsca  # å¸æ”¶æ•ˆç‡
        
        eta_total += eta * w
        rho_total += rho * w
    
    # ç°¡åŒ– PSF åƒæ•¸ï¼ˆå‡è¨­å–®æ®µæ ¸ï¼‰
    sigma_core = 15.0 * (550.0 / wavelength_nm) ** 0.8
    kappa_tail = 40.0 * (550.0 / wavelength_nm) ** 0.6
    core_ratio = 0.7
    
    return {
        'sigma_core': sigma_core,
        'kappa_tail': kappa_tail,
        'core_ratio': core_ratio,
        'eta_scatter': eta_total
    }


# ============================================================
# 3. æ’å€¼å‡½æ•¸ï¼ˆèˆ‡å¯¦éš›ä½¿ç”¨ä¸€è‡´ï¼‰
# ============================================================

def interpolate_mie_params(wavelength_nm, iso, lookup_data):
    """é›™ç·šæ€§æ’å€¼ï¼ˆèˆ‡å¯¦éš›ç¨‹å¼ç¢¼ä¸€è‡´ï¼‰"""
    wavelengths = lookup_data['wavelengths']
    iso_values = lookup_data['iso_values']
    
    # æ‰¾åˆ°æ³¢é•·æ’å€¼ç´¢å¼•
    if wavelength_nm <= wavelengths[0]:
        w_idx0, w_idx1 = 0, 0
        w_frac = 0.0
    elif wavelength_nm >= wavelengths[-1]:
        w_idx0, w_idx1 = len(wavelengths) - 1, len(wavelengths) - 1
        w_frac = 0.0
    else:
        w_idx1 = np.searchsorted(wavelengths, wavelength_nm)
        w_idx0 = w_idx1 - 1
        w_frac = (wavelength_nm - wavelengths[w_idx0]) / (wavelengths[w_idx1] - wavelengths[w_idx0])
    
    # æ‰¾åˆ° ISO æ’å€¼ç´¢å¼•
    if iso <= iso_values[0]:
        i_idx0, i_idx1 = 0, 0
        i_frac = 0.0
    elif iso >= iso_values[-1]:
        i_idx0, i_idx1 = len(iso_values) - 1, len(iso_values) - 1
        i_frac = 0.0
    else:
        i_idx1 = np.searchsorted(iso_values, iso)
        i_idx0 = i_idx1 - 1
        i_frac = (iso - iso_values[i_idx0]) / (iso_values[i_idx1] - iso_values[i_idx0])
    
    # é›™ç·šæ€§æ’å€¼
    def bilinear(arr):
        v00 = arr[w_idx0, i_idx0]
        v01 = arr[w_idx0, i_idx1]
        v10 = arr[w_idx1, i_idx0]
        v11 = arr[w_idx1, i_idx1]
        
        v0 = v00 * (1 - i_frac) + v01 * i_frac
        v1 = v10 * (1 - i_frac) + v11 * i_frac
        
        return v0 * (1 - w_frac) + v1 * w_frac
    
    return {
        'sigma_core': bilinear(lookup_data['sigma_core']),
        'kappa_tail': bilinear(lookup_data['kappa_tail']),
        'core_ratio': bilinear(lookup_data['core_ratio']),
        'eta_scatter': bilinear(lookup_data['eta_scatter'])
    }


# ============================================================
# 4. é©—è­‰æ¸¬è©¦é›†
# ============================================================

def generate_test_grid():
    """
    ç”Ÿæˆæ¸¬è©¦æ ¼é»ï¼ˆåŒ…å«æ¥µç«¯æƒ…æ³ï¼‰
    
    ç­–ç•¥ï¼š
    1. æŸ¥è¡¨æ ¼é»ï¼ˆæ‡‰è©²å®Œå…¨åŒ¹é…ï¼Œèª¤å·® <0.01%ï¼‰
    2. ä¸­é–“é»ï¼ˆæ¸¬è©¦æ’å€¼ç²¾åº¦ï¼‰
    3. æ¥µç«¯é»ï¼ˆé‚Šç•Œæ¸¬è©¦ï¼‰
    """
    test_cases = []
    
    # é¡å‹ 1ï¼šæŸ¥è¡¨æ ¼é»ï¼ˆ20 å€‹é»ï¼Œé æœŸèª¤å·® ~0%ï¼‰
    wavelengths_grid = np.linspace(400, 700, 10)
    iso_grid = [50, 100, 400, 800, 3200, 6400]
    
    for wl in wavelengths_grid:
        for iso in iso_grid:
            test_cases.append({
                'wavelength': wl,
                'iso': iso,
                'type': 'grid_point'
            })
    
    # é¡å‹ 2ï¼šä¸­é–“é»ï¼ˆ100 å€‹é»ï¼Œæ¸¬è©¦æ’å€¼ï¼‰
    for _ in range(100):
        wl = np.random.uniform(420, 680)
        iso = np.random.choice([75, 150, 500, 1200, 2500, 5000])
        test_cases.append({
            'wavelength': wl,
            'iso': iso,
            'type': 'interpolation'
        })
    
    # é¡å‹ 3ï¼šæ¥µç«¯é»ï¼ˆé‚Šç•Œæ¸¬è©¦ï¼‰
    extreme_cases = [
        (400, 50, 'extreme_min_min'),
        (700, 6400, 'extreme_max_max'),
        (400, 6400, 'extreme_min_iso_max'),
        (700, 50, 'extreme_max_wavelength_min_iso'),
        (550, 6400, 'extreme_high_iso'),
        (400, 50, 'extreme_low_iso'),
    ]
    
    for wl, iso, label in extreme_cases:
        test_cases.append({
            'wavelength': wl,
            'iso': iso,
            'type': label
        })
    
    return test_cases


# ============================================================
# 5. åŸ·è¡Œé©—è­‰
# ============================================================

def validate_comprehensive():
    """å…¨é¢é©—è­‰ Mie æŸ¥è¡¨"""
    print("=" * 60)
    print("Mie æ•£å°„æŸ¥è¡¨å…¨é¢é©—è­‰")
    print("=" * 60)
    print()
    
    # è¼‰å…¥æŸ¥è¡¨
    print("ğŸ“‚ è¼‰å…¥æŸ¥è¡¨æ•¸æ“š...")
    lookup_data = load_mie_lookup()
    print(f"   - æ³¢é•·ç¯„åœ: {lookup_data['wavelengths'][0]:.0f}-{lookup_data['wavelengths'][-1]:.0f} nm")
    print(f"   - ISO ç¯„åœ: {lookup_data['iso_values'][0]:.0f}-{lookup_data['iso_values'][-1]:.0f}")
    print(f"   - æ ¼é»æ•¸: {len(lookup_data['wavelengths'])} Ã— {len(lookup_data['iso_values'])} = {len(lookup_data['wavelengths']) * len(lookup_data['iso_values'])}")
    print()
    
    # ç”Ÿæˆæ¸¬è©¦é›†
    print("ğŸ§ª ç”Ÿæˆæ¸¬è©¦é›†...")
    test_cases = generate_test_grid()
    print(f"   - ç¸½æ¸¬è©¦é»: {len(test_cases)}")
    print(f"   - æ ¼é»: {sum(1 for t in test_cases if t['type'] == 'grid_point')}")
    print(f"   - æ’å€¼é»: {sum(1 for t in test_cases if t['type'] == 'interpolation')}")
    print(f"   - æ¥µç«¯é»: {sum(1 for t in test_cases if 'extreme' in t['type'])}")
    print()
    
    # åŸ·è¡Œé©—è­‰
    print("âš™ï¸  è¨ˆç®—æ’å€¼èª¤å·®ï¼ˆé€™éœ€è¦å¹¾åˆ†é˜ï¼‰...")
    errors = []
    extreme_errors = {}
    
    for i, case in enumerate(test_cases):
        if (i + 1) % 20 == 0:
            print(f"   é€²åº¦: {i+1}/{len(test_cases)} ({100*(i+1)/len(test_cases):.1f}%)")
        
        # è¨ˆç®— Ground Truth
        gt = compute_mie_parameters_ground_truth(case['wavelength'], case['iso'])
        
        # è¨ˆç®—æ’å€¼å€¼
        interp = interpolate_mie_params(case['wavelength'], case['iso'], lookup_data)
        
        # è¨ˆç®—ç›¸å°èª¤å·®ï¼ˆ%ï¼‰
        eta_error = abs(interp['eta_scatter'] - gt['eta_scatter']) / (gt['eta_scatter'] + 1e-10) * 100
        
        errors.append({
            'wavelength': case['wavelength'],
            'iso': case['iso'],
            'type': case['type'],
            'eta_error': eta_error,
            'eta_gt': gt['eta_scatter'],
            'eta_interp': interp['eta_scatter']
        })
        
        # è¨˜éŒ„æ¥µç«¯æƒ…æ³
        if 'extreme' in case['type']:
            extreme_errors[case['type']] = eta_error
    
    print()
    
    # ============================================================
    # 6. çµ±è¨ˆåˆ†æ
    # ============================================================
    
    eta_errors = [e['eta_error'] for e in errors]
    
    print("=" * 60)
    print("ğŸ“Š çµ±è¨ˆçµæœ")
    print("=" * 60)
    print()
    print("æ•£å°„æ•ˆç‡ (Î·) æ’å€¼èª¤å·®:")
    print(f"   - å¹³å‡å€¼ (Mean):        {np.mean(eta_errors):.4f}%")
    print(f"   - ä¸­ä½æ•¸ (Median):      {np.median(eta_errors):.4f}%")
    print(f"   - æ¨™æº–å·® (Std):         {np.std(eta_errors):.4f}%")
    print(f"   - 95th ç™¾åˆ†ä½æ•¸:        {np.percentile(eta_errors, 95):.4f}%")
    print(f"   - 99th ç™¾åˆ†ä½æ•¸:        {np.percentile(eta_errors, 99):.4f}%")
    print(f"   - æœ€å¤§å€¼ (Max):         {np.max(eta_errors):.4f}%")
    print()
    
    # æ¥µç«¯æƒ…æ³å ±å‘Š
    print("ğŸ”´ æ¥µç«¯æƒ…æ³èª¤å·®:")
    for label, error in extreme_errors.items():
        status = "âœ…" if error < 5.0 else "âš ï¸"
        print(f"   {status} {label:30s}: {error:.4f}%")
    print()
    
    # åˆ†é¡çµ±è¨ˆ
    grid_errors = [e['eta_error'] for e in errors if e['type'] == 'grid_point']
    interp_errors = [e['eta_error'] for e in errors if e['type'] == 'interpolation']
    
    print("ğŸ“ˆ åˆ†é¡çµ±è¨ˆ:")
    print(f"   æ ¼é»èª¤å·® (æ‡‰æ¥è¿‘ 0):    {np.mean(grid_errors):.4f}% (max: {np.max(grid_errors):.4f}%)")
    print(f"   æ’å€¼é»èª¤å·®:             {np.mean(interp_errors):.4f}% (max: {np.max(interp_errors):.4f}%)")
    print()
    
    # èƒ½é‡å®ˆæ†é©—è­‰
    print("ğŸ”‹ èƒ½é‡å®ˆæ†é©—è­‰ (Î· â‰¤ 1.0):")
    violations = [e for e in errors if e['eta_gt'] > 1.0 or e['eta_interp'] > 1.0]
    if violations:
        print(f"   âš ï¸  ç™¼ç¾ {len(violations)} å€‹é•åèƒ½é‡å®ˆæ†çš„é»")
        for v in violations[:5]:
            print(f"      - Î»={v['wavelength']:.0f}nm, ISO={v['iso']:.0f}: Î·_gt={v['eta_gt']:.3f}, Î·_interp={v['eta_interp']:.3f}")
    else:
        print("   âœ… æ‰€æœ‰é»ç¬¦åˆèƒ½é‡å®ˆæ†")
    print()
    
    # å–®èª¿æ€§é©—è­‰
    print("ğŸ“Š å–®èª¿æ€§é©—è­‰ (ISO å¢åŠ  â†’ Î· å¢åŠ ):")
    monotonic_violations = 0
    for wl in [450, 550, 650]:
        iso_seq = [100, 400, 800, 3200]
        eta_seq = []
        for iso in iso_seq:
            interp = interpolate_mie_params(wl, iso, lookup_data)
            eta_seq.append(interp['eta_scatter'])
        
        is_monotonic = all(eta_seq[i] <= eta_seq[i+1] for i in range(len(eta_seq)-1))
        status = "âœ…" if is_monotonic else "âš ï¸"
        print(f"   {status} Î»={wl}nm: {eta_seq}")
        
        if not is_monotonic:
            monotonic_violations += 1
    
    if monotonic_violations == 0:
        print("   âœ… æ‰€æœ‰æ³¢é•·ç¬¦åˆå–®èª¿æ€§")
    print()
    
    # ============================================================
    # 7. ç”Ÿæˆå ±å‘Š
    # ============================================================
    
    report_path = Path(__file__).parent.parent / "validation_report.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("Mie æ•£å°„æŸ¥è¡¨é©—è­‰å ±å‘Š\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"å¹³å‡èª¤å·®: {np.mean(eta_errors):.4f}%\n")
        f.write(f"95th ç™¾åˆ†ä½: {np.percentile(eta_errors, 95):.4f}%\n")
        f.write(f"æœ€å¤§èª¤å·®: {np.max(eta_errors):.4f}%\n\n")
        f.write("æ¥µç«¯æƒ…æ³:\n")
        for label, error in extreme_errors.items():
            f.write(f"  {label}: {error:.4f}%\n")
    
    print(f"ğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜: {report_path}")
    
    # ============================================================
    # 8. ç¹ªè£½èª¤å·®åˆ†å¸ƒåœ–
    # ============================================================
    
    try:
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # å­åœ– 1: èª¤å·®ç›´æ–¹åœ–
        axes[0, 0].hist(eta_errors, bins=50, alpha=0.7, color='steelblue', edgecolor='black')
        axes[0, 0].axvline(np.mean(eta_errors), color='red', linestyle='--', label=f'Mean: {np.mean(eta_errors):.2f}%')
        axes[0, 0].axvline(np.percentile(eta_errors, 95), color='orange', linestyle='--', label=f'95th: {np.percentile(eta_errors, 95):.2f}%')
        axes[0, 0].set_xlabel('Interpolation Error (%)')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title('Error Distribution')
        axes[0, 0].legend()
        axes[0, 0].grid(alpha=0.3)
        
        # å­åœ– 2: èª¤å·® vs ISO
        iso_vals = [e['iso'] for e in errors]
        axes[0, 1].scatter(iso_vals, eta_errors, alpha=0.5, s=10)
        axes[0, 1].set_xlabel('ISO')
        axes[0, 1].set_ylabel('Error (%)')
        axes[0, 1].set_title('Error vs ISO')
        axes[0, 1].set_xscale('log')
        axes[0, 1].grid(alpha=0.3)
        
        # å­åœ– 3: èª¤å·® vs æ³¢é•·
        wl_vals = [e['wavelength'] for e in errors]
        axes[1, 0].scatter(wl_vals, eta_errors, alpha=0.5, s=10)
        axes[1, 0].set_xlabel('Wavelength (nm)')
        axes[1, 0].set_ylabel('Error (%)')
        axes[1, 0].set_title('Error vs Wavelength')
        axes[1, 0].grid(alpha=0.3)
        
        # å­åœ– 4: CDF
        sorted_errors = np.sort(eta_errors)
        cdf = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)
        axes[1, 1].plot(sorted_errors, cdf, linewidth=2)
        axes[1, 1].axhline(0.95, color='orange', linestyle='--', label='95th percentile')
        axes[1, 1].axvline(np.percentile(eta_errors, 95), color='orange', linestyle='--')
        axes[1, 1].set_xlabel('Error (%)')
        axes[1, 1].set_ylabel('CDF')
        axes[1, 1].set_title('Cumulative Distribution')
        axes[1, 1].legend()
        axes[1, 1].grid(alpha=0.3)
        
        plt.tight_layout()
        
        plot_path = Path(__file__).parent.parent / "error_distribution.png"
        plt.savefig(plot_path, dpi=150)
        print(f"ğŸ“Š èª¤å·®åˆ†å¸ƒåœ–å·²ä¿å­˜: {plot_path}")
    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•ç”Ÿæˆåœ–è¡¨ï¼ˆéœ€è¦ matplotlibï¼‰: {e}")
    
    print()
    print("=" * 60)
    print("âœ… é©—è­‰å®Œæˆ")
    print("=" * 60)


# ============================================================
# 9. ä¸»ç¨‹å¼
# ============================================================

if __name__ == "__main__":
    validate_comprehensive()
